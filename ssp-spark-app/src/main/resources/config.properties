#    Stephen McMullan x19139497@student.ncirl.ie
#
#    SSP Spark Data Processing Application
#
#    Processes stream from Kafka, aggregates and persists aggregate records to Elasticsearch

# DEBUG mode
# This is for running the Spark application within the IDE for debug purposes
# You also need to remove the "provided" scope from the spark-core and spark-sql libraries in the Maven pom.xml
# Also the /etc/hosts file on the host machine needs to be edited to add "kafka" as an alias to localhost to allow the
# IDE run application to connect to the broker
run.ide = false

# Kafka connection properties (Data Source)
kafka.server = kafka:9092
kafka.topics = telecom_trento
kafka.topic.starting.offset = earliest
kafka.max.offsets.per.trigger = 1000000

# Elasticsearch connection properties (Data Sink)
es.server = elasticsearch
es.port = 9200
es.scheme = http
es.index = sparkcdr

# Aggregation Window settings (Tumbling Window)
time.window.secs = 60

# Aggregation time enablement
enable.hourly.agg = true
enable.daily.agg = true
enable.weekly.agg = true