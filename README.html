<h1>Scalable Systems Programming Project</h1>

<p>Stephen McMullan (x19139497@student.ncirl.ie)</p>

<p>Semester 3, Scalable Systems Programming, Postgraduate Diploma in Data Analytics</p>

<p>National College of Ireland</p>

<h2>GitHub Project Code Repository</h2>

<p><a href="https://github.com/smcmullan-ncirl/SSPProject">https://github.com/smcmullan-ncirl/SSPProject</a></p>

<h2>Datasets</h2>

<p>https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/QLCABU</p>

<p>[1]Telecom Italia, ‘Telecommunications - SMS, Call, Internet - TN’. Harvard Dataverse, 2015, doi: 10.7910/DVN/QLCABU.</p>

<p>The repository for the data is at the following location on AWS:</p>

<p>https://s3.console.aws.amazon.com/s3/buckets/x19139497/Telecommunications%2520-%2520SMS%252C%2520Call%252C%2520Internet%2520-%2520TN/?region=eu-west-1&amp;tab=overview</p>

<h3>About the Telecom Italia Big Data Challenge dataset</h3>

<p>https://www.nature.com/articles/sdata201555</p>

<p>[1]G. Barlacchi et al., ‘A multi-source dataset of urban life in the city of Milan and the Province of Trentino’, 
Scientific Data, vol. 2, no. 1, Art. no. 1, Oct. 2015, doi: 10.1038/sdata.2015.55.</p>

<h2>Software Application Description</h2>

<p>The software consists of three applications:</p>

<ol>
<li><p><strong>SSP Data Import</strong>: this Java application uses the AWS SDK to stream CSV files containing the Telecom Data from AWS 
S3 stored as objects in a S3 bucket, transforms them to JSON format and sends them as a producer to a topic in Apache 
Kafka message broker. This application runs on a Java Virtual Machine runtime environment.</p></li>
<li><p><strong>SSP Spark Application</strong>: this Scala application is written using the Spark API and deploys to the Apache Spark 
runtime platform. It acts as an Apache Kafka topic consumer, processes the topic messages in JSON format and convert 
them to Scala objects using Jackson Object Mapping, provides Map/Reduce functionality to aggregate the totals for 
Calls In / Calls Out / SMS In / SMS Out and writes out the aggregated record to an Elasticsearch index which is then 
queried using Kibana user interface for data representation.</p></li>
<li><p><strong>SSP Flink Application</strong>: this Scala application is written using the Flink API and deploys to the Apache Flink 
runtime platform. It acts as an Apache Kafka topic consumer, processes the topic messages in JSON format and convert 
them to Scala objects using Jackson Object Mapping, provides Map/Reduce functionality to aggregate the totals for 
Calls In / Calls Out / SMS In / SMS Out and writes out the aggregated record to an Elasticsearch index which is then 
queried using Kibana user interface for data representation.</p></li>
</ol>

<h3>Setup Build Environment Instructions</h3>

<pre><code>sudo apt-get install openjdk-8-jdk
sudo update-alternatives --config java (choose JDK8)
sudo apt-get install maven
sudo apt install git

git clone https://github.com/smcmullan-ncirl/SSPProject.git (alternatively unzip the provided ZIP archive)

cd SSPProject
mvn clean package
</code></pre>

<h2>Runtime Environment Description</h2>

<p>The runtime environment consists of the following:</p>

<ol>
<li><p><strong>Amazon Web Services</strong>: The Telecom Italia data is stored in objects inside an AWS S3 bucket</p></li>
<li><p><strong>Openstack</strong>:</p></li>
<li><p><strong>Apache Kafka</strong>:</p></li>
<li><p><strong>Apache Spark</strong>:</p></li>
<li><p><strong>Apache Flink</strong></p></li>
<li><p><strong>Elasticsearch</strong>:</p></li>
<li><p><strong>Kibana</strong>:</p></li>
</ol>

<p>JDK8 is the preferred build environment</p>

<p>The target platform is Apache Spark 2.4.6 (limited by Elasticsearch integration) and Apache Flink 1.10</p>

<h4>Architecture Diagram</h4>

<h1>Overall Application Build and Deployment</h1>

<h2>Setup Runtime Environment Instructions</h2>

<p>First install Docker and add your user to the docker group</p>

<pre><code>sudo apt install docker.io
sudo usermod -aG docker ${USER}
sudo apt install docker-compose
</code></pre>

<p>You need to logout and log back in to take on the new group assignment</p>

<p>You can then check if Docker is running correctly by running the Hello World container</p>

<pre><code>docker run hello-world
</code></pre>

<h2>Run Instructions</h2>

<p>Add the following to /etc/hosts to allow the Kafka and Elasticsearch container to be addressable by the IDE, 
SSPDataImport and the Spark Worker</p>

<pre><code>sudo vi /etc/hosts

127.0.0.1   localhost kafka elasticsearch
</code></pre>

<p>Then bring up the Docker environment</p>

<pre><code>docker-compose up -d
docker-compose ps
</code></pre>

<h2>Kafka Topic Creation (Optional)</h2>

<p>I've provided a script to create all topics associated with this project</p>

<pre><code>createKafkaTopics.sh
</code></pre>

<p>This combines the following commands for each topic and then a general listing at the end:</p>

<pre><code>docker-compose exec kafka kafka-topics.sh --create --replication-factor 1 --partitions 1 --bootstrap-server localhost:9092 --topic &lt;topic-name&gt;

docker-compose exec kafka kafka-topics.sh --list --bootstrap-server localhost:9092
</code></pre>

<p>However it isn't strictly necessary to do this as the topics are created by the broker when the client producer
sends messages to those topics if they do not exist previously.</p>

<h2>Kafka Consumer Instructions (Optional)</h2>

<p>If you want to monitor the records being produced into any Kafka topic then you can start the Kafka console consumer as follows:</p>

<pre><code>docker-compose exec kafka kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic &lt;topic name&gt;
</code></pre>

<p>The topics can be read from the beginning by the consumer like this:</p>

<pre><code>docker-compose exec kafka kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic ping --from-beginning
</code></pre>

<h1>SSPDataImport - The Data Collection Processor application</h1>

<h2>Starting Processing Instructions</h2>

<pre><code>cd ssp-data-import
</code></pre>

<p>and then</p>

<pre><code>mvn exec:java
</code></pre>

<p>or</p>

<pre><code>java -jar ssp-data-import/target/ssp-data-import-jar-with-dependencies.jar
</code></pre>

<p>This will start the processing of the data records from the Open Mobile Performance dataset to the Kafka broker by default.</p>

<h3>Configuration Options</h3>

<p>If you would like to enable persistence of the source dataset records to Elasticsearch to allow inspection using Kibana
then you need to edit the following file and rebuild the project and re-run the application as described above:</p>

<pre><code>ssp-data-import/src/main/resources/config.properties
</code></pre>

<p>The configuration properties are:</p>

<pre><code>aws.bucketname = x19139497
aws.object.prefix = Telecommunications - SMS, Call, Internet - TN/sms-call-internet-tn-

kafka.persist = true
kafka.server = localhost:9092
kafka.topic = telecom_trento

es.persist = false
es.server = localhost
es.port = 9200
es.scheme = http
es.index = dataimportcdr
</code></pre>

<p>There's no real need to change any settings</p>

<h2>Performance</h2>

<p>The following metrics were acquired running the SSPDataImport application on the machine specification listed above
without any modifications to the configuration file supplied i.e. it processes the entire dataset into the Kafka broker
in 7029 seconds ~ <strong>2 hours</strong></p>

<h1>SSPSparkApp - The Spark Streaming Processor application</h1>

<h2>Building and deploying the application to Spark</h2>

<p>After building the project with Maven</p>

<pre><code>cd SSPProject
mvn clean package
</code></pre>

<p>The Spark application will be packaged in a JAR file in:</p>

<pre><code>SSPProject/ssp-spark-app/target/ssp-spark-app.jar
</code></pre>

<p>The application can be deployed to Spark with the following commands:</p>

<pre><code>cd SSPProject/ssp-spark-app/target
docker cp ssp-spark-app.jar sspproject_spark-master_1:/
docker cp ssp-spark-app.jar sspproject_spark-worker_1:/

docker exec -it sspproject_spark-master_1 bin/spark-submit -v \
--master spark://spark-master:7077 \
--deploy-mode cluster \
--class ie.ncirl.sspproject.dataprocess.SSPSparkApp \
file:///ssp-spark-app.jar
</code></pre>

<h3>Spark deployment troubleshooting</h3>

<p>If there are issues submitting the application you will see some output from the spark-submit command like this:</p>

<pre><code>log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.NativeCodeLoader).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
20/04/18 18:32:34 INFO SecurityManager: Changing view acls to: spark
20/04/18 18:32:34 INFO SecurityManager: Changing modify acls to: spark
20/04/18 18:32:34 INFO SecurityManager: Changing view acls groups to: 
20/04/18 18:32:34 INFO SecurityManager: Changing modify acls groups to: 
20/04/18 18:32:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
20/04/18 18:32:34 INFO Utils: Successfully started service 'driverClient' on port 40787.
20/04/18 18:32:34 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.5:7077 after 40 ms (0 ms spent in bootstraps)
20/04/18 18:32:34 INFO ClientEndpoint: Driver successfully submitted as driver-20200418183234-0001
20/04/18 18:32:34 INFO ClientEndpoint: ... waiting before polling master for driver state
20/04/18 18:32:39 INFO ClientEndpoint: ... polling master for driver state
20/04/18 18:32:39 INFO ClientEndpoint: State of **driver-20200418183234-0001** is **FAILED**
20/04/18 18:32:39 INFO ShutdownHookManager: Shutdown hook called
20/04/18 18:32:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-8f3e37a1-ab9e-4a96-b8ed-59d18210b805
</code></pre>

<p>If something like this occurs then check the following location:</p>

<pre><code>docker exec -it sspproject_spark-worker_1 bash
cd /opt/bitnami/spark/work/
ls -lrtd driver*
cd &lt;last driver directory&gt;
cat stderr
</code></pre>

<h3>Spark UI</h3>

<p>When the application has been deployed it can be monitored using the Spark UI</p>

<pre><code>http://localhost:8080/
</code></pre>

<h3>Spark logs</h3>

<p>The Spark logs can be checked as follows:</p>

<pre><code>docker exec -it sspproject_spark-master_1 bash
    cd /opt/bitnami/spark/work/
    ls -lrtd driver*
    cd &lt;last driver directory&gt;
    cat stderr
</code></pre>

<p>The logs are also viewable via the Spark UI at the link given above</p>

<h1>SSPFlinkApp - The Flink Streaming Processor application</h1>

<h2>Building and deploying the application to Flink</h2>

<p>After building the project with Maven</p>

<pre><code>cd SSPProject
mvn clean package
</code></pre>

<p>The Flink application will be packaged in a JAR file in:</p>

<pre><code>SSPProject/ssp-flink-app/target/ssp-flink-app.jar
</code></pre>

<p>The application can be deployed to Spark with the following commands:</p>

<pre><code>cd SSPProject/ssp-flink-app/target
docker cp ssp-flink-app.jar sspproject_flink-jobmanager_1:/

docker exec -it sspproject_flink-jobmanager_1 ./bin/flink run -c \
ie.ncirl.sspproject.dataprocess.SSPFlinkApp /ssp-flink-app.jar
</code></pre>

<h3>Flink deployment troubleshooting</h3>

<h3>Flink UI</h3>

<pre><code>http://localhost:9081
</code></pre>

<h3>Flink logs</h3>

<h1>SSPElasticApp - The Elasticsearch/Kibana Processor application</h1>

<h2>Building and deploying the application to Elasticsearch/Kibana</h2>

<h3>Elasticsearch/Kibana deployment troubleshooting</h3>

<h3>Elasticsearch/Kibana UI</h3>

<pre><code>http://localhost:5601/app/kibana
</code></pre>

<h3>Elasticsearch/Kibana logs</h3>

<h1>Environment Shutdown Instructions</h1>

<h2>Kafka Topic Deletion (Optional)</h2>

<p>You can remove the topics from the Kafka with this command:</p>

<pre><code>docker-compose exec kafka kafka-topics.sh --delete --bootstrap-server localhost:9092 --topic &lt;topic name&gt;
</code></pre>

<p>I've provided a script to remove all topics associated with this project</p>

<pre><code>deleteKafkaTopics.sh
</code></pre>

<h2>Stopping the Docker containers</h2>

<pre><code>docker-compose down
docker-compose ps
</code></pre>

<p><strong>TBD</strong>: Note that when the Docker environment is stopped and restarted the Kafka broker topics are no longer present. This
requires further investigation.</p>

<h2>Cleanup Instructions</h2>

<p>A lot of diskspace can be consumed over time with Docker with containers, images and volumes. 
Some of the useful commands to see what is active and to remove it are as follows:</p>

<pre><code>docker ps
docker stop &lt;container name&gt;
docker rm &lt;container name&gt;
docker images
docker rmi &lt;image name&gt;
</code></pre>

<p>and finally Docker volumes under /var/lib/docker/volumes can be removed with:</p>

<pre><code>docker system prune --all --volumes
</code></pre>

<h2>IDE Development and Debugging</h2>

<p>There is a property in the SSPSparkApp config.properties file called "run.ide" which needs to be set to true. You also
have to remove the "provided" scope from the spark-core and spark-sql libraries in the Maven pom.xml for the ssp-spark-app
module.</p>

<p>The effect of these changes allow the application to be run outside the deployed Spark cluster in "local" mode</p>

<p>JetBrains IntelliJ IDE with the Scala plugin is recommended for development.</p>

<p>The application was developed and tested on the following configuration:</p>

<pre><code>1. VM: Oracle VirtualBox 6.1
2. Linux OS: Mint Linux 20
3. RAM: 16GB
4. CPU: 2 logical processors
5. Disk Space: 50GB
</code></pre>

<p>The deployment of the Docker containers take approximately 3GB of disk storage</p>

<p>The clone of the GitHub repo is approx 7MB of storage</p>

<p>The local Maven repository generated by the build consumes approx 360MB of storage</p>

<h2>Links</h2>

<h3>Wurtmeister Docker images for Kafka</h3>

<pre><code>http://wurstmeister.github.io/kafka-docker/
https://github.com/wurstmeister/kafka-docker/blob/master/README.md
https://github.com/wurstmeister/kafka-docker/wiki/Connectivity
</code></pre>

<h3>Bitnami Docker images for Spark</h3>

<pre><code>https://github.com/bitnami/bitnami-docker-spark
</code></pre>

<h3>Kakfa - Spark integration</h3>

<pre><code>https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html
https://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html
</code></pre>

<h3>Kakfa - Flink integration</h3>

<pre><code>https://ci.apache.org/projects/flink/flink-docs-stable/dev/connectors/kafka.html
</code></pre>

<h3>Elasticsearch Java High Level REST Client</h3>

<pre><code>https://www.elastic.co/guide/en/elasticsearch/client/java-rest/7.8/java-rest-high.html
</code></pre>
