# Data Intensive Architecture Project

Stephen McMullan (x19139497@student.ncirl.ie)

Semester 2, Data Intensive Architectures, Postgraduate Diploma in Data Analytics

National College of Ireland

## GitHub Project Code Repository

https://github.com/smcmullan-ncirl/DIAProject

## About the Open Mobile Performance Dataset

https://www.measurementlab.net/

https://www.measurementlab.net/tests/mobiperf/

## Datasets

The dataset contains the following quantities of measurements by type:

    ping : 3066411
    traceroute : 369750
    http : 819611
    dns_lookup : 1009149
    udp_burst : 13307
    tcpthroughput : 266445
    context : 291
    myspeedtest_ping : 1
    myspeedtestdns_lookup : 2
    device_info : 2681
    network_info : 2661
    battery_info : 2657
    ping_test : 3
    sim_info : 2651
    state_info : 2650
    usage_info : 2644
    rrc : 10846
    PageLoadTime : 1
    pageloadtime : 14169
    video : 3679
    sequential : 5163
    quic-http : 402207
    cronet-http : 401105
    multipath_latency : 2163380
    multipath_http : 2931

The repository for the data is at the following location on the Google Cloud Platform:

    https://console.cloud.google.com/storage/browser/openmobiledata_public
    
There is some description of the data schema associated with each measurement type in the GitHub repository of the
MobiPerf application here:

    https://github.com/Mobiperf/MobiPerf/blob/master/README
    
However it is far from definitive. This application and test measurement is defunct and there is evidence to suggest
that the application was modified by independent research teams with new measurement types beyond what the GitHub
repository for the application describes and the back-end M-Lab data repository used to collect in resulting in the
overall dataset above. The schema of some of the measurement types above can be deduced from parameter key naming but
the rest is pure guesswork.

## Prerequisites and resource usage

The application was developed and tested on the following configuration:

    1. VM: Oracle VirtualBox 6.0
    2. Linux OS: Mint Linux 19.3
    3. RAM: 16GB
    4. CPU: 2 logical processors
    5. Disk Space: 50GB
    
The deployment of the Docker containers take approximately 3GB of disk storage

The clone of the GitHub repo is approx 7MB of storage

The local Maven repository generated by the build consumes approx 360MB of storage

Post build the application working directory is approx 33MB of storage

Processing the entire dataset into the Kafka broker consumes approx **20GB** of storage

## Architecture Diagram

![DIA Project Architecture](DIAProjectArch.png)
 	
# Overall Application Build and Deployment

## Setup Build Environment Instructions

    sudo apt-get install openjdk-8-jdk
    sudo update-alternatives --config java (choose JDK8)
    sudo apt-get install maven
    sudo apt install git
    git clone https://github.com/smcmullan-ncirl/DIAProject.git
    cd DIAProject
    mvn clean package
    
JDK8 is the preferred build environment and there is a dependency on Scala 2.11 for the Spark application.
    
The target platform is Apache Spark 2.4.5

TBD: Full Java 11 and Scala 2.12 support is coming with Apache Spark 3.0 which is still at pre-release status currently

## IDE Development and Debugging

There is a property in the DIASparkApp config.properties file called "run.ide" which needs to be set to true. You also
have to remove the "provided" scope from the spark-core and spark-sql libraries in the Maven pom.xml for the dia-spark-app
module.

The effect of these changes allow the application to be run outside the deployed Spark cluster in "local" mode

JetBrains IntelliJ IDE with the Scala plugin is recommended for development.

## Setup Runtime Environment Instructions

First install Docker and add your user to the docker group

    sudo apt install docker.io
    sudo usermod -aG docker ${USER}
    sudo apt install docker-compose

You need to logout and log back in to take on the new group assignment

You can then check if Docker is running correctly by running the Hello World container
    
    docker run hello-world

## Run Instructions

    docker-compose up -d
    docker-compose ps

## Kafka Topic Creation (Optional)

I've provided a script to create all topics associated with this project

    createKafkaTopics.sh
    
This combines the following commands for each topic and then a general listing at the end:

    docker-compose exec kafka kafka-topics.sh --create --replication-factor 1 --partitions 1 --bootstrap-server localhost:9092 --topic <topic-name>
    
    docker-compose exec kafka kafka-topics.sh --list --bootstrap-server localhost:9092
    
However it isn't strictly necessary to do this as the topics are created by the broker when the client producer
sends messages to those topics if they do not exist previously.

The topic names are the same of the measurement types listed above.

## Kafka Consumer Instructions (Optional)

If you want to monitor the records being produced into any Kafka topic then you can start the Kafka console consumer as follows:

    docker-compose exec kafka kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic <topic name>
    
The topics can be read from the beginning by the consumer like this:

    docker-compose exec kafka kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic ping --from-beginning

## Postgres Setup Instructions (Optional)

If you choose to persist data to PostgreSQL then installing the command line SQL client is very useful:

    sudo apt-get install postgresql-client
    sudo apt install postgresql-client-common
    
DBVizualizer is recommended if you would like a full GUI

# DIADataImport - The Data Collection Processor application

## Starting Processing Instructions

    cd dia-data-import
    
and then

    mvn exec:java
    
or

    java -jar dia-data-import/target/dia-data-import-jar-with-dependencies.jar
    
This will start the processing of the data records from the Open Mobile Performance dataset to the Kafka broker by default.

### Configuration Options
If you would like to enable/disable Kafka, PostgreSQL or CSV processing then you need to edit the following file and rebuild the project and re-run the application as described above:

    dia-data-import/src/main/resources/config.properties
    
The configuration properties are:

    gcp.bucketname = openmobiledata_public

    tempfile.dir = /tmp

    file.offset.min = 0
    file.offset.max = -1

    ping = true
    traceroute = true
    http = true
    dns_lookup = true
    udp_burst = true
    tcpthroughput = true
    context = true
    myspeedtest_ping = true
    myspeedtestdns_lookup = true
    device_info = true
    network_info = true
    battery_info = true
    ping_test = true
    sim_info = true
    state_info = true
    usage_info = true
    rrc = true
    PageLoadTime = true
    pageloadtime = true
    video = true
    sequential = true
    quic-http = true
    cronet-http = true
    multipath_latency = true
    multipath_http = true

    kafka.enabled = true
    kafka.server = localhost:9092

    db.enabled = false
    db.url = jdbc:postgresql://localhost:5432/diadb
    db.user = diauser
    db.password = diapassword
    db.schema = bigdata

    csv.enabled = false
    csv.file.prefix = openmobiledata_

There's no real need to change any settings apart from the enabled flags

**TBD**: Note that in the case of writing out to CSV and PostgresSQL DB there is only full implementation support for
three types at the moment and thus the other types will need to be disabled in the config file above to suppress
exceptions and error handling messages:

    ping
    http
    tcpthroughput

## Performance

The following metrics were acquired running the DIADataImport application on the machine specification listed above
without any modifications to the configuration file supplied i.e. it processes the entire dataset into the Kafka broker
in 7029 seconds ~ **2 hours**

![DIADataImport Performance](DIADataImportPerf.png)

# DIASparkApp - The Data Aggregation Processor application

## Building and deploying the application to Spark

After building the project with Maven

    cd DIAProject
    mvn clean package
    
The Spark application will be packaged in a JAR file in:

    DIAProject/dia-spark-app/target/dia-spark-app.jar
    
The application can be deployed to Spark with the following commands:

    cd DIAProject/dia-spark-app/target
    docker cp dia-spark-app.jar diaproject_spark-master_1:/
    docker cp dia-spark-app.jar diaproject_spark-worker_1:/
        
    docker exec -it diaproject_spark-master_1 bin/spark-submit -v \
    --master spark://localhost:7077 \
    --deploy-mode cluster \
    --class ie.ncirl.diaproject.dataprocess.DIASparkApp \
    file:///dia-spark-app.jar

**TBD**: Note that in the case of Spark application processing there is only full implementation support for
three types at the moment (the other types are disabled in the config.properties file assosciated with the
application:

    ping
    http
    tcpthroughput
    
### Spark deployment troubleshooting

If there are issues submitting the application you will see some output from the spark-submit command like this:

    log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.NativeCodeLoader).
    log4j:WARN Please initialize the log4j system properly.
    log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
    Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
    20/04/18 18:32:34 INFO SecurityManager: Changing view acls to: spark
    20/04/18 18:32:34 INFO SecurityManager: Changing modify acls to: spark
    20/04/18 18:32:34 INFO SecurityManager: Changing view acls groups to: 
    20/04/18 18:32:34 INFO SecurityManager: Changing modify acls groups to: 
    20/04/18 18:32:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
    20/04/18 18:32:34 INFO Utils: Successfully started service 'driverClient' on port 40787.
    20/04/18 18:32:34 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.5:7077 after 40 ms (0 ms spent in bootstraps)
    20/04/18 18:32:34 INFO ClientEndpoint: Driver successfully submitted as driver-20200418183234-0001
    20/04/18 18:32:34 INFO ClientEndpoint: ... waiting before polling master for driver state
    20/04/18 18:32:39 INFO ClientEndpoint: ... polling master for driver state
    20/04/18 18:32:39 INFO ClientEndpoint: State of **driver-20200418183234-0001** is **FAILED**
    20/04/18 18:32:39 INFO ShutdownHookManager: Shutdown hook called
    20/04/18 18:32:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-8f3e37a1-ab9e-4a96-b8ed-59d18210b805

If something like this occurs then check the following location:

    docker exec -it diaproject_spark-worker_1 bash
    cd /opt/bitnami/spark/work/
    ls -lrtd driver*
    cd <last driver directory>
    cat stderr
    
### Spark UI

When the application has been deployed it can be monitored using the Spark UI

    http://localhost:8080/
    
### Spark logs

The Spark logs can be checked as follows:

    docker exec -it diaproject_spark-master_1 bash
    
# Environment Shutdown Instructions

## Kafka Topic Deletion (Optional)

You can remove the topics from the Kafka with this command:

    docker-compose exec kafka kafka-topics.sh --delete --bootstrap-server localhost:9092 --topic <topic name>
    
I've provided a script to remove all topics associated with this project

    deleteKafkaTopics.sh

## Stopping the Docker containers

    docker-compose down
    docker-compose ps
    
**TBD**: Note that when the Docker environment is stopped and restarted the Kafka broker topics are no longer present. This
requires further investigation.

## Cleanup Instructions

A lot of diskspace can be consumed over time with Docker with containers, images and volumes. 
Some of the useful commands to see what is active and to remove it are as follows:

    docker ps
    docker stop <container name>
    docker rm <container name>
    docker images
    docker rmi <image name>
    
and finally Docker volumes under /var/lib/docker/volumes can be removed with:

    docker system prune --all --volumes

## Links

Wurtmeister Docker images for Kafka

    http://wurstmeister.github.io/kafka-docker/
    https://github.com/wurstmeister/kafka-docker/blob/master/README.md
    https://github.com/wurstmeister/kafka-docker/wiki/Connectivity
    
Bitnami Docker images for Spark

    https://github.com/bitnami/bitnami-docker-spark

Kakfa - Spark integration

    https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html
    https://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html
    
Complex dataset processing using the Spark API

    https://docs.databricks.com/spark/latest/dataframes-datasets/complex-nested-data.html

