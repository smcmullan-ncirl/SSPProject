# DEBUG mode
# This is for running the Spark application within the IDE for debug purposes
# You also need to remove the "provided" scope from the spark-core and spark-sql libraries in the Maven pom.xml
# Also the /etc/hosts file on the host machine needs to be edited to add "kafka" as an alias to localhost to allow the
# IDE run application to connect to the broker
run.ide = false

kafka.server = kafka:9092
kafka.topics = ping,\
tcpthroughput,\
http

# Not supported by the SSP Spark Application at present
# Implementation requires creation of the case classes specific to the measurement type in the
# ie.ncirl.sspproject.dataprocess.measurement package in ssp-spark-app

#dns_lookup,\
#udp_burst,\
#traceroute,\
#context,\
#myspeedtest_ping,\
#myspeedtestdns_lookup,\
#device_info,\
#network_info,\
#battery_info,\
#ping_test,\
#sim_info,\
#state_info,\
#usage_info,\
#rrc,\
#PageLoadTime,\
#pageloadtime,\
#video,\
#sequential,\
#quic-http,\
#cronet-http,\
#multipath_latency,\
#multipath_http

kafka.topic.starting.offset = earliest
kafka.topic.ending.offset = latest

# It is worth changing this property to the number of CPUs you have available across your cluster
# Make sure it reflects the SPARK_WORKER_CORES environment setting in docker-compose.yml
spark.partitions = 2