# DEBUG mode
# This is for running the Spark application within the IDE for debug purposes
# You also need to remove the "provided" scope from the spark-core and spark-sql libraries in the Maven pom.xml
# Also the /etc/hosts file on the host machine needs to be edited to add "kafka" as an alias to localhost to allow the
# IDE run application to connect to the broker
run.ide = false

kafka.server = kafka:9092
kafka.topics = telecom_trento
kafka.topic.starting.offset = latest
kafka.max.offsets.per.trigger = 1000

# It is worth changing this property to the number of CPUs you have available across your cluster
# Make sure it reflects the SPARK_WORKER_CORES environment setting in docker-compose.yml
spark.partitions = 2

es.server = elasticsearch
es.port = 9200
es.index = sparkcdr

time.window.secs = 60