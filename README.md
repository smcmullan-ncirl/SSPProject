# Scalable Systems Programming Project

Stephen McMullan (x19139497@student.ncirl.ie)

Semester 3, Scalable Systems Programming, Postgraduate Diploma in Data Analytics

National College of Ireland

## GitHub Project Code Repository

    https://github.com/smcmullan-ncirl/SSPProject

## About the Telecom Italia Big Data Challenge dataset

    https://www.nature.com/articles/sdata201555

[1]G. Barlacchi et al., ‘A multi-source dataset of urban life in the city of Milan and the Province of Trentino’, 
Scientific Data, vol. 2, no. 1, Art. no. 1, Oct. 2015, doi: 10.1038/sdata.2015.55.


## Datasets

https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/QLCABU

[1]Telecom Italia, ‘Telecommunications - SMS, Call, Internet - TN’. Harvard Dataverse, 2015, doi: 10.7910/DVN/QLCABU.

The repository for the data is at the following location on AWS:

    https://s3.console.aws.amazon.com/s3/buckets/x19139497/Telecommunications%2520-%2520SMS%252C%2520Call%252C%2520Internet%2520-%2520TN/?region=eu-west-1&tab=overview


## Prerequisites and resource usage

The application was developed and tested on the following configuration:

    1. VM: Oracle VirtualBox 6.1
    2. Linux OS: Mint Linux 20
    3. RAM: 16GB
    4. CPU: 2 logical processors
    5. Disk Space: 50GB
    
The deployment of the Docker containers take approximately 3GB of disk storage

The clone of the GitHub repo is approx 7MB of storage

The local Maven repository generated by the build consumes approx 360MB of storage

Post build the application working directory is approx 33MB of storage

Processing the entire dataset into the Kafka broker consumes approx **20GB** of storage

## Architecture Diagram

 	
# Overall Application Build and Deployment

## Setup Build Environment Instructions

    sudo apt-get install openjdk-11-jdk
    sudo update-alternatives --config java (choose JDK11)
    sudo apt-get install maven
    sudo apt install git
    git clone https://github.com/smcmullan-ncirl/SSPProject.git
    cd SSPProject
    export MAVEN_OPTS="-Xss4m"
    mvn clean package
    
JDK11 is the preferred build environment and there is a dependency on Scala 2.12 for the Spark application.
    
The target platform is Apache Spark 3.0.0 and Apache Flink 1.11

## IDE Development and Debugging

There is a property in the SSPSparkApp config.properties file called "run.ide" which needs to be set to true. You also
have to remove the "provided" scope from the spark-core and spark-sql libraries in the Maven pom.xml for the ssp-spark-app
module.

The effect of these changes allow the application to be run outside the deployed Spark cluster in "local" mode

JetBrains IntelliJ IDE with the Scala plugin is recommended for development.

## Setup Runtime Environment Instructions

First install Docker and add your user to the docker group

    sudo apt install docker.io
    sudo usermod -aG docker ${USER}
    sudo apt install docker-compose

You need to logout and log back in to take on the new group assignment

You can then check if Docker is running correctly by running the Hello World container
    
    docker run hello-world

## Run Instructions

Add the following to /etc/hosts to allow the Kafka container to be addressable by the IDE, SSPDataImport and the Spark
Worker

    sudo vi /etc/hosts
    
    127.0.0.1	localhost kafka
    
Then bring up the Docker environment

    docker-compose up -d
    docker-compose ps

## Kafka Topic Creation (Optional)

I've provided a script to create all topics associated with this project

    createKafkaTopics.sh
    
This combines the following commands for each topic and then a general listing at the end:

    docker-compose exec kafka kafka-topics.sh --create --replication-factor 1 --partitions 1 --bootstrap-server localhost:9092 --topic <topic-name>
    
    docker-compose exec kafka kafka-topics.sh --list --bootstrap-server localhost:9092
    
However it isn't strictly necessary to do this as the topics are created by the broker when the client producer
sends messages to those topics if they do not exist previously.

The topic names are the same of the measurement types listed above.

## Kafka Consumer Instructions (Optional)

If you want to monitor the records being produced into any Kafka topic then you can start the Kafka console consumer as follows:

    docker-compose exec kafka kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic <topic name>
    
The topics can be read from the beginning by the consumer like this:

    docker-compose exec kafka kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic ping --from-beginning

## Postgres Setup Instructions (Optional)

If you choose to persist data to PostgreSQL then installing the command line SQL client is very useful:

    sudo apt-get install postgresql-client
    sudo apt install postgresql-client-common
    
DBVizualizer is recommended if you would like a full GUI

# SSPDataImport - The Data Collection Processor application

## Starting Processing Instructions

    cd ssp-data-import
    
and then

    mvn exec:java
    
or

    java -jar ssp-data-import/target/ssp-data-import-jar-with-dependencies.jar
    
This will start the processing of the data records from the Open Mobile Performance dataset to the Kafka broker by default.

### Configuration Options
If you would like to enable/disable Kafka, PostgreSQL or CSV processing then you need to edit the following file and rebuild the project and re-run the application as described above:

    ssp-data-import/src/main/resources/config.properties
    
The configuration properties are:

    aws.bucketname = x19139497
    aws.object.prefix = Telecommunications - SMS, Call, Internet - TN/sms-call-internet-tn-
    kafka.server = localhost:9092
    kafka.topic = telecom_trento

There's no real need to change any settings

## Performance

The following metrics were acquired running the SSPDataImport application on the machine specification listed above
without any modifications to the configuration file supplied i.e. it processes the entire dataset into the Kafka broker
in 7029 seconds ~ **2 hours**

# SSPSparkApp - The Data Aggregation Processor application

## Building and deploying the application to Spark

After building the project with Maven

    cd SSPProject
    mvn clean package
    
The Spark application will be packaged in a JAR file in:

    SSPProject/ssp-spark-app/target/ssp-spark-app.jar
    
The application can be deployed to Spark with the following commands:

    cd SSPProject/ssp-spark-app/target
    docker cp ssp-spark-app.jar sspproject_spark-master_1:/
    docker cp ssp-spark-app.jar sspproject_spark-worker_1:/
        
    docker exec -it sspproject_spark-master_1 bin/spark-submit -v \
    --master spark://localhost:7077 \
    --deploy-mode cluster \
    --class ie.ncirl.sspproject.dataprocess.SSPSparkApp \
    file:///ssp-spark-app.jar
    
### Spark deployment troubleshooting

If there are issues submitting the application you will see some output from the spark-submit command like this:

    log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.NativeCodeLoader).
    log4j:WARN Please initialize the log4j system properly.
    log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
    Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
    20/04/18 18:32:34 INFO SecurityManager: Changing view acls to: spark
    20/04/18 18:32:34 INFO SecurityManager: Changing modify acls to: spark
    20/04/18 18:32:34 INFO SecurityManager: Changing view acls groups to: 
    20/04/18 18:32:34 INFO SecurityManager: Changing modify acls groups to: 
    20/04/18 18:32:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(spark); groups with view permissions: Set(); users  with modify permissions: Set(spark); groups with modify permissions: Set()
    20/04/18 18:32:34 INFO Utils: Successfully started service 'driverClient' on port 40787.
    20/04/18 18:32:34 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.5:7077 after 40 ms (0 ms spent in bootstraps)
    20/04/18 18:32:34 INFO ClientEndpoint: Driver successfully submitted as driver-20200418183234-0001
    20/04/18 18:32:34 INFO ClientEndpoint: ... waiting before polling master for driver state
    20/04/18 18:32:39 INFO ClientEndpoint: ... polling master for driver state
    20/04/18 18:32:39 INFO ClientEndpoint: State of **driver-20200418183234-0001** is **FAILED**
    20/04/18 18:32:39 INFO ShutdownHookManager: Shutdown hook called
    20/04/18 18:32:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-8f3e37a1-ab9e-4a96-b8ed-59d18210b805

If something like this occurs then check the following location:

    docker exec -it sspproject_spark-worker_1 bash
    cd /opt/bitnami/spark/work/
    ls -lrtd driver*
    cd <last driver directory>
    cat stderr
    
### Spark UI

When the application has been deployed it can be monitored using the Spark UI

    http://localhost:8080/
    
### Spark logs

The Spark logs can be checked as follows:

    docker exec -it sspproject_spark-master_1 bash
        cd /opt/bitnami/spark/work/
        ls -lrtd driver*
        cd <last driver directory>
        cat stderr
        
The logs are also viewable via the Spark UI at the link given above
    
# Environment Shutdown Instructions

## Kafka Topic Deletion (Optional)

You can remove the topics from the Kafka with this command:

    docker-compose exec kafka kafka-topics.sh --delete --bootstrap-server localhost:9092 --topic <topic name>
    
I've provided a script to remove all topics associated with this project

    deleteKafkaTopics.sh

## Stopping the Docker containers

    docker-compose down
    docker-compose ps
    
**TBD**: Note that when the Docker environment is stopped and restarted the Kafka broker topics are no longer present. This
requires further investigation.

## Cleanup Instructions

A lot of diskspace can be consumed over time with Docker with containers, images and volumes. 
Some of the useful commands to see what is active and to remove it are as follows:

    docker ps
    docker stop <container name>
    docker rm <container name>
    docker images
    docker rmi <image name>
    
and finally Docker volumes under /var/lib/docker/volumes can be removed with:

    docker system prune --all --volumes

## Links

### Wurtmeister Docker images for Kafka

    http://wurstmeister.github.io/kafka-docker/
    https://github.com/wurstmeister/kafka-docker/blob/master/README.md
    https://github.com/wurstmeister/kafka-docker/wiki/Connectivity
    
### Bitnami Docker images for Spark

    https://github.com/bitnami/bitnami-docker-spark

### Kakfa - Spark integration

    https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html
    https://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html

### Kakfa - Flink integration

    https://ci.apache.org/projects/flink/flink-docs-stable/dev/connectors/kafka.html
