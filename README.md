# Data Intensive Architecture Project

Stephen McMullan (x19139497@student.ncirl.ie)

Semester 2, Data Intensive Architectures, Postgraduate Diploma in Data Analytics

National College of Ireland

## GitHub Project Code Repository

https://github.com/smcmullan-ncirl/DIAProject

## About the Open Mobile Performance Dataset

https://www.measurementlab.net/

https://www.measurementlab.net/tests/mobiperf/

## Datasets

The dataset contains the following quantities of measurements by type:

    ping : 3066411
    traceroute : 369750
    http : 819611
    dns_lookup : 1009149
    udp_burst : 13307
    tcpthroughput : 266445
    context : 291
    myspeedtest_ping : 1
    myspeedtestdns_lookup : 2
    device_info : 2681
    network_info : 2661
    battery_info : 2657
    ping_test : 3
    sim_info : 2651
    state_info : 2650
    usage_info : 2644
    rrc : 10846
    PageLoadTime : 1
    pageloadtime : 14169
    video : 3679
    sequential : 5163
    quic-http : 402207
    cronet-http : 401105
    multipath_latency : 2163380
    multipath_http : 2931

https://console.cloud.google.com/storage/browser/openmobiledata_public

## Prerequisites and resource usage

The application was developed and tested on the following configuration:

    1. VM: Oracle VirtualBox 6.0
    2. Linux OS: Mint Linux 19.3
    3. RAM: 16GB
    4. CPU: 2 logical processors
    5. Disk Space: 50GB
    
The deployment of the Docker containers take approximately 3GB of disk storage

The clone of the GitHub repo is approx 7MB of storage

The local Maven repository generated by the build consumes 350MB of storage

Post build the application working directory is approx 33MB of storage

Processing the entire dataset into the Kafka broker consumes approx 20GB of storage

## Architecture Diagram

![DIA Project Architecture](DIAProjectArch.png)
 	
# Overall Application Build and Deployment

## Setup Build Environment Instructions

    sudo apt-get install maven
    sudo apt install git
    git clone https://github.com/smcmullan-ncirl/DIAProject.git
    cd DIAProject
    mvn clean package

## Setup Runtime Environment Instructions

First install Docker and add your user to the docker group

    sudo apt install docker.io
    sudo usermod -aG docker ${USER}
    sudo apt install docker-compose

You need to logout and log back in to take on the new group assignment

You can then check if Docker is running correctly by running the Hello World container
    
    docker run hello-world

## Run Instructions

    docker-compose up -d
    docker-compose ps

## Kafka Topic Creation (Optional)

I've provided a script to create all topics associated with this project

    createKafkaTopics.sh
    
This combines the following commands for each topic and then a general listing at the end:

    docker-compose exec kafka kafka-topics.sh --create --replication-factor 1 --partitions 1 --bootstrap-server localhost:9092 --topic <topic-name>
    
    docker-compose exec kafka kafka-topics.sh --list --bootstrap-server localhost:9092
    
However it isn't strictly necessary to do this as the topics are created by the broker when the client producer sends messages to those topics if they do not exist previously.

## Kafka Consumer Instructions (Optional)

If you want to monitor the records being produced into any Kafka topic then you can start the Kafka console consumer as follows:

    docker-compose exec kafka kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic <topic name>

## Postgres Setup Instructions (Optional)

If you choose to persist data to PostgreSQL then installing the command line SQL client is very useful:

    sudo apt-get install postgresql-client
    sudo apt install postgresql-client-common
    
DBVizualizer is recommended if you would like a full GUI

# GCPDataImport - The Data Collection Processor application

## Starting Processing Instructons

    cd dia-data-import
    
and then

    mvn exec:java
    
or

    java -jar dia-data-import/target/gcpdataimport-jar-with-dependencies.jar
    
This will start the processing of the data records from the Open Mobile Performance dataset to the Kafka broker by default.

### Configuration Options
If you would like to enable/disable Kafka, PostgreSQL or CSV processing then you need to edit the following file and rebuild the project and re-run the application as described above:

    dia-data-import/src/main/resources/config.properties
    
The configuration properties are:

    gcp.bucketname = openmobiledata_public

    tempfile.dir = /tmp

    file.offset.min = 0
    file.offset.max = -1

    ping = true
    traceroute = true
    http = true
    dns_lookup = true
    udp_burst = true
    tcpthroughput = true
    context = true
    myspeedtest_ping = true
    myspeedtestdns_lookup = true
    device_info = true
    network_info = true
    battery_info = true
    ping_test = true
    sim_info = true
    state_info = true
    usage_info = true
    rrc = true
    PageLoadTime = true
    pageloadtime = true
    video = true
    sequential = true
    quic-http = true
    cronet-http = true
    multipath_latency = true
    multipath_http = true

    kafka.enabled = true
    kafka.server = localhost:9092

    db.enabled = false
    db.url = jdbc:postgresql://localhost:5432/diadb
    db.user = diauser
    db.password = diapassword
    db.schema = bigdata

    csv.enabled = false
    csv.file.prefix = openmobiledata_

There's no real need to change any settings apart from the enabled flags

## Performance

The following metrics were acquired running the GCPDataImport application on the machine specification listed above
without any modifications to the configuration file supplied i.e. it processes the entire dataset into the Kafka broker
in 7029 seconds ~ 2 hours 

![GCPDataImport Performance](GCPDataImportPerf.png)

# DIASparkApp - The Data Aggregation Processor application

## Building and deploying the application to Spark

After building the project with Maven

    cd DIAProject
    mvn clean package
    
The Spark application will be packaged in a JAR file in:

    DIAProject/dia-spark-app/target/dia-spark-app-1.0-SNAPSHOT.jar
    
The application can be deployed to Spark with the following commands:

    cd DIAProject/dia-spark-app/target
    docker cp dia-spark-app-1.0-SNAPSHOT.jar diaproject_spark-master_1:/
    
    docker exec -it diaproject_spark-master_1 bin/spark-submit -v \
    --master spark://localhost:7077 \
    --deploy-mode cluster \
    --class ie.ncirl.diaproject.dataprocess.DIASparkApp \
    file:///dia-spark-app-1.0-SNAPSHOT.jar

### Spark UI

When the application has been deployed it can be monitored using the Spark UI

    http://localhost:8080/
    
### Spark logs

The Spark logs can be checked as follows:

    docker exec -it diaproject_spark-master_1 bash
    

# Environment Shutdown Instructions

## Kafka Topic Deletion (Optional)

You can remove the topics from the Kafka with this command:

    docker-compose exec kafka kafka-topics.sh --delete --bootstrap-server localhost:9092 --topic <topic name>
    
I've provided a script to remove all topics associated with this project

    deleteKafkaTopics.sh

## Stopping the Docker containers

    docker-compose down
    docker-compose ps

## Cleanup Instructions

A lot of diskspace can be consumed over time with Docker with containers, images and volumes. Some of the useful commands to see what is active and to remove it are as follows:

    docker ps
    docker stop <container name>
    docker rm <container name>
    docker images
    docker rmi <image name>
    
and finally Docker volumes under /var/lib/docker/volumes can be removed with:

    docker system prune --all --volumes

## Links

    https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html
    https://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html
    https://docs.databricks.com/spark/latest/dataframes-datasets/complex-nested-data.html
